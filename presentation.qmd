---
title: "A/B Tests: The Essentials"
author: "Lachlan Deer"
institute: "Social Media and Web Analytics, Spring 2025"
execute: 
  cache: true
#urlcolor: blue
format: 
  beamer: 
    aspectratio: 32
    navigation: horizontal
    theme: cousteau
---

```{r, echo=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(broom)
library(infer)
library(vtable)
library(car)
```

## Learning Goals 

1. Explain the basic principles of an A/B Test
2. Analyze A/B test data to draw causal conclusions about a treatment
3. Determine the appropriate sample size for an experiment
4. Discuss challenges of shifting to an "experimentation first" company culture

## Where Are We Now? 

So far we've discussed:

* What makes a **good research question**
* The importance of **research design** and thinking through the **identification** problem to find the "right variation" to estimate casual effects
* **Randomized Control Trials** as a means to generate the right variation

Today: A/B tests $\leftrightarrow$ Randomized Control Trials online!

* aka Online Controlled Experiments

## A/B Tests: The Basic Idea 

\begin{figure}
\includegraphics[width=9cm]{figs/ab_test_basic.png}
\end{figure}

Source: Kohavi (2019)

## Example: Bing Ads with Site Links

\begin{center}
\textbf{Shound Bing add site links to ads that allow advertisers to offer multiple destinations on an ad?}
\end{center}

\begin{figure}
\includegraphics[width=12cm]{figs/ab_test_ex_1.png}
\end{figure}


**\alert{Question:}** What are the pros and cons of each design?

**\alert{Question:}** Which one created more revenue for Bing?

<!---
PRO - richer ads, users better informed
con: can display less ads -- 3 vs 4, how fast page loads

better? B, increased annual revenue by 100m US
--->


## Example: Bing Search with Underlined Links

\begin{center}
\textbf{Does underlining a link impact clickthrough?}
\end{center}

\begin{figure}
\includegraphics[width=12cm]{figs/ab_test_ex_2.png}
\end{figure}

**\alert{Question:}** Which one created more revenue for Bing?

<!---
A was better
underlines improve CTR for ads and organic so more revenue

remark: most web designs dont have underlines despite lower CTR and longer time to click
--->

# Working Example: Email Marketing

## An Email A/B Test

The email A/B test we will analyze was conducted by an online wine store. 

\begin{figure}
\includegraphics[width=10cm]{figs/wine_store.png}
\end{figure}

Source: Total Wine & More

## Wine retailer email test

**Test setting**: email to retailer email list

**Unit**: email address

**Treatments**: email version A, email version B, holdout

**Reponse**: open, click on link and 1-month purchase ($)

**Selection**: all active customers

**Assignment**: randomly assigned (1/3 each) 

## Loading & Inspecting the Data
\footnotesize

```{r, message=FALSE}
df <- read_csv("data/test_data.csv")

glimpse(df)
```

## Variables associated with the Test 

**Treatment indicator ($T_i$)**

- Which (randomized) treatment was received  
     
**Outcomes ($Y_i$)**

- Outcome(s) measured for each customer, i.e. the outcome variable
    
**Baseline variables ($Z_i$)**

- Other stuff we know about customers **prior** to the randomization
- Sometimes called "pre-randomization covariates" or "observables"

**\alert{Question:}** For each variable in the dataset, which one of these categories does it fall into?

# Analysis of A/B tests

## The First Question

\begin{center}
\textbf{\alert{What is the first question you should ask about an A/B test?}}
\end{center}

## The First Question

\begin{center}
\textbf{\alert{What is the first question you should ask about an A/B test?}}
\end{center}

~~Did the treatment affect the response?~~

Was the randomization done correctly? 

\begin{center}
\textbf{\alertb{How can we check randomization with the data at hand?}} 
\end{center}

## Randomization checks

**Randomization checks** confirm that the **baseline variables** are **distributed similarly** for the **treatment and control groups**. 

* Also known as "**\alert{Balance tests}**"

## Randomization checks: Our data
\footnotesize

```{r, eval = TRUE}
df %>%
    select(group, days_since, visits, past_purch, 
           chard, sav_blanc, syrah, cab) %>%
    group_by(group) %>%
    summarize(across(everything(), list(mean = mean)))

```

## Randomization checks

We can **test for balance** across treatments for each of our baseline variables: 

```{r, echo = TRUE, eval = FALSE}
# note: output omitted
df %>%
    select(group, days_since, visits, past_purch, 
           chard, sav_blanc, syrah, cab) %>%
    st(group = 'group', group.test = TRUE)
```

## Randomization checks 

\begin{center}
\textbf{\alert{Randomization seems to check out!}}
\end{center}

\vspace{2cm}

... onto average treatment effects


## Did the treatments affect the responses?

Look at the means of outcome variables between treatments:

```{r}
df %>%
    select(group, open, click, purch) %>%
    group_by(group) %>%
    summarize(across(everything(), list(mean = mean)))
```

**\alert{Question:}** What differences do you observe?

<!---
* Email A looks better for opens and clicks. 
* Both emails seem to generate higher average purchases than the control.
--->

##

```{r, fig.align='center', out.width="10cm"}
df %>%
    filter(group != 'ctrl') %>%
    mutate(click = as.factor(click)) %>%
    ggplot() + 
    geom_bar(aes(fill = click, y=as.numeric(click), x=group),
             position="fill", stat="identity") +
    theme_bw()
```

## Does email A have higher open rate than B?

```{r}
df %>%
    filter(group != "ctrl") %>%
    mutate(open = as.logical(open)) %>%
    prop_test(open ~ group,
              alternative = "greater")

```

## Does email A have higher open rate than B?

```{r}
mod <- lm(open ~ group, 
          data = df %>% filter(group != "ctrl")
          )
tidy(mod)
```

## Does email A have higher click rate than B?

```{r}
mod <- lm(click ~ group, 
          data = df %>% filter(group != "ctrl")
          )
tidy(mod)
```

## Does email A lead to higher average purchases than B?

```{r}
mod <- lm(purch ~ group, 
          data = df %>% filter(group != "ctrl")
          )
tidy(mod)
```

## Do the emails lead to higher average purchases?

```{r}
mod <- lm(purch ~ group, 
          data = df
          )
tidy(mod)
```


## Does email A lead to higher average purchases than B?

```{r}
linearHypothesis(mod, c("groupemail_A = groupemail_B"))
```

## Summary of findings 

Email A has significantly higher opens and clicks than email B, 

* But purchase are similar for both emails $\rightarrow$ Send email A!

Both emails generate higher average purchases than the control $\rightarrow$ Send emails! 

# Design of A/B tests

## Seven key questions 

1. Business question
2. Test setting (lab vs. field)
3. Unit of analysis (visit, customer, store)
4. Treatments
5. Response variable(s)
6. Selection of units
7. Assignment to treatments
8. Sample size

\begin{center}
\textbf{\alert{If you can answer these questions, you have a test plan}}
\end{center}

## Email test

**Business questions**: Does email work? If so which email is better? 

**Test setting**: email to retailer customers

**Unit**: email address

**Treatments**: email version A, email version B, holdout

**Reponse**: open, click and 30-day purchase ($)

**Selection**: all active emails on email list (open in last 12 months)

**Assignment**: randomly assigned (1/3 each)

**Sample size**: 123,988 emails

## Sample size planning

The standard recommendation is to set the sample size **in advance** and not test for significance until the data comes in.


* The recommended sample size is: 

$$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$$

## Interpreting the sample size formula

$$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$$  

- More noise, $s^2$ $\rightarrow$ larger sample size
- Smaller difference to detect, $d$ $\rightarrow$ larger sample size
- Lower error rates, $(z_{1-\alpha/2} + z_\beta)$ $\rightarrow$ larger sample size



## Sample size planning: Key ideas

**Data is noisy**, so the group with the higher average in the test not always have the higher true response.   

There are **\alert{two mistakes}** you can make: 

* **Type I error**:  Declare the treatments different, when they are the same ($\alpha$)
* **Type II error**: Declare the treatment the same, when they are different ($\beta$)

I want a low probability of both of those mistakes ($\alpha$, $\beta$) given a specific known difference between treatments ($d$) and noise in my response ($s$)

$$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$$ 

## Sample size calculator in R

Sample size to detect at \$1 difference in average 30-day purchases:

```{r, eval=FALSE, echo = TRUE}
power.t.test(sd = sd(df$purch), # ideally using 
                                # pre-experiment data!
             delta = 1, # minimum detectable effect
             sig.level = 0.95, # alpha: industry standard
             power=0.80 # 1 - beta: industry standard 
             )
```

## Sample size planning

* **\alert{Continous response}** (e.g. money, time on website) 

$$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 s^2}{d^2} \right)$$

* **\alert{Binary response}** (e.g. conversions) 

$$n_1 = n_2 \approx (z_{1-\alpha/2} + z_\beta)^2 \left( \frac{2 p (1-p)}{d^2} \right)$$


## Sample size calculator in R

**Binary response**

```{r, eval=FALSE, echo = TRUE}
power.prop.test(p1=0.07, 
                p2=0.07 + 0.01, # d = 0.01
                sig.level=0.05, 
                power=0.80
                )
```

## A word of caution about sample size calculators

There are **different sample size formulas floating around**. 

* These formulas differ on what assumptions they may about what you are trying to do,
* It **can be very hard to figure out what assumptions are being made** 
* ... even for experts 
* So use some care before plugging numbers into an online calculator

A sample size calculation will help you identify the right amount of data you need for the problem at hand. 

## Choosing Outcome Variables

Agreeing on **outcome variables** is **not** as **easy** as it sounds

* Should be defined using short-term metrics that predict long-term value 
* (and hard to game)
* Think about customer lifetime value, not immediate revenue

* Use few but key metrics
Conversion funnels use Pirate metrics: AARRR: acquisition, activation, retention, revenue, and referral

## Most Ideas Fail

Experiments at Microsoft ([paper](https://exp-platform.com/Documents/ExP_DMCaseStudies.pdf)):

* 1/3 of ideas were positive ideas and statistically significant
* 1/3 of ideas were flat, with no statistically significant difference
* 1/3 of ideas were negative and statistically significant

At Bing (well optimized), the success rate is lower: 10-20%.

Implication: **Aim for small continuous improvements**

## Twyman's Law

\begin{center}
\alertb{Any figure that looks interesting or different is usually wrong}
\end{center}

* Check before celebrating

## Cultural Challenges

\begin{center}
"Experimentation is the least arrogant method of gaining knowledge" 

- Isaac Asimov
\end{center}

Some folks believe controlled experiments threaten their jobs

* "we know what to do and we're sure of it"
* Reflex-like rejection of new knowledge because it contradicts entrenched norms, beliefs or paradigm


## Ethical Issues

**Controversy in treatment design**

* Facebook's [emotional contagion](https://www.pnas.org/doi/full/10.1073/pnas.1320040111) experiment
* Amazon and early [pricing experiments](https://www.bizjournals.com/seattle/stories/2000/09/25/daily21.html)
* OK Cupid (Tinder for the previous generation) with [deception on match score](https://theblog.okcupid.com/we-experiment-on-human-beings-5dd9fe280cd5)

**\alert{Minimal Risk Experimentation}**:

> "the probability and magnitude of harm or discomfort anticipated in the research are not greater in and of themselves than those ordinarily encountered in daily life or during the performance of routine physical or psychological examinations or tests"

When in doubt have an Institutional Review Board

# Recap

## Summary

* A/B testing is running Randomized Control Trials online
* Balance tests help confirm that randomization into treatment is indeed random
* Statistical inference toolkit and linear regression enable us to estimate the treatment effects
* The correct sample size for detecting a treatment effect is a crucial aspect of test design
* There are challenges beyond the analysis of data that are important obstacles in implementation
    
## Acknowledgements

I have borrowed content and inspiration from the following sources:

* Elea Feit's ["Advanced A/B testing workshop"](https://eleafeit.github.io/ab_test/)

* Ronny Kohavi's ["A/B Testing at Scale: Accelerating Software Innovation"](https://www.researchgate.net/publication/333071058_AB_Testing_at_Scale_Accelerating_Software_Innovation)

## License & Citation
\small
Suggested Citation:

```{r, engine='out', echo=TRUE, eval = FALSE}
@misc{smwa2025_abtest,
      title={"Social Media and Web Analytics: A/B Tests - Basics"},
      author={Lachlan Deer},
      year={2025},
      url = "https://tisem-digital-marketing.github.io/2025-smwa"
}
```

This course adheres to the principles of the [\alertb{Open Science Community of Tilburg University}](https://www.tilburguniversity.edu/research/open-science-community). 
This initiative advocates for transparency and accessibility in research and teaching to all levels of society and thus creating more accountability and impact.

This work is licensed under a [\alertb{Creative Commons Attribution-ShareAlike 4.0 International License}](http://creativecommons.org/licenses/by-sa/4.0/).